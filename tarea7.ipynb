{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehUxc7AtDfU5"
   },
   "source": [
    "### ITCR - Escuela de Computación\n",
    "### Curso IC-6200 - Inteligencia Artificial\n",
    "### Aprendizaje supervisado\n",
    "\n",
    "### Redes de memoria de corto y largo plazo con PyTorch \n",
    "### (Long-Short Term Memory Networks-LSTM)\n",
    "\n",
    "**Profesora: María Auxiliadora Mora**\n",
    "\n",
    "### Tarea corta #7\n",
    "\n",
    "### Estudiantes:\n",
    "**1. Pablo Alberto Muñoz Hidalgo**\n",
    "\n",
    "**2. Luis Andrés Rojas Murillo**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyr84u8ab1kw"
   },
   "source": [
    "## Introducción\n",
    "\n",
    "La clasificación de textos y el reconocimiento de entidades nombradas (Named Entity Recognition o NER por sus siglas en inglés) son técnicas fundamentales que constituyen el primer paso en muchas tareas de Procesamiento de lenguaje natural (NLP). NER, es un área de investigación relacionada a la extracción de información, que permite localizar y clasificar nombres de entidades que se encuentran en texto libre, en categorías comunmente organizaciones, lugares, tiempo, personas, entre otros. Ejemplo:\n",
    "\n",
    "- El fundador de [Microsoft Corporation] (organización), [Bill Gates] (persona), comentó que se abrirán 1000 puestos de trabajo en la [Región Chorotega] (lugar) a partir del año 2022 (fecha).  \n",
    "\n",
    "La clasificación de textos permite categorizar el contenido asociando este a un conjunto de etiquetas predefinidas o clases. Su uso más popular es el análisis de sentimientos. Ejemplo:\n",
    "\n",
    "- En mi opinión, la película fue muy buena porque pudo dar a conocer a los espectadores cómo puede afectar una situación traumática a la mente humana. (Clase = 5 o excelente). \n",
    "\n",
    "Las redes neuronales recurrentes o RNN (Rumelhart et al., 1986, como se citó en LeCun et al., 2015) son una familia de redes neuronales para el procesamiento de secuencias de datos, las cuales en un tiempo t, reciben el estado anterior, es decir, su salida en el tiempo t podría usarse como insumo del procesamiento de la siguiente entrada, de modo que la información pueda propagarse a medida que la red pasa por la secuencia de entrada. Las redes Long Short-Term Memory (LSTM) son un tipo de red neuronal recurrente capaz de aprender dependencias a largo plazo.\n",
    "\n",
    "El siguiente ejemplo implementa NER con una LSTM para etiquetar el rol que juegan las palabras en las oraciones. \n",
    "\n",
    "\n",
    "## Ejemplo\n",
    "\n",
    "El sistema implementado en el código adjunto soluciona el problema de estimar el rol de una palabra en una frase, por ejempo roles como determinante (DET), nombre (NN) y verbo (V). \n",
    "Ejemplo para la frase:\n",
    "\n",
    "- \"El perro come manzana\" la salida deberá ser: [\"DET\", \"NN\", \"V\", \"NN\"]). \n",
    "\n",
    "Este proceso se conoce en el procesamiento de lenguaje natural como \"part of speech tagging (POS)\".\n",
    "\n",
    "Este es un ejemplo simple con datos introducidos en el código basado en [1].\n",
    "\n",
    "Se realizarán los siguientes pasos\n",
    "\n",
    "   * Definición de los ejemplos (codificados) \n",
    "   * Preprocesamiento de las palabras a clasificar\n",
    "   * Definición del modelo\n",
    "   * Instanciación del modelo, definición de la función de pérdida y del optimizador  \n",
    "   * Entrenamiento de la red\n",
    "   * Pruebas del modelo resultante con unos cuantos ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gmqx6kMSDfU9",
    "outputId": "c6a51d34-4aeb-4cd4-ed22-b92d297ea46e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x261f4765cd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bibliotecas requeridas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "#Import scikit learn metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsFmju8xDfVQ"
   },
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    \"\"\"\n",
    "    Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    Parámetros: \n",
    "        x: vector con los datos. \n",
    "    Salida: \n",
    "        out: valor \n",
    "        inds: índice\n",
    "    \"\"\"\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "    \n",
    "\n",
    "    # Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"\n",
    "    Retorna un tensor con los indices del diccionario para cada palabras en una oración.\n",
    "    Parámetros:\n",
    "       seq: oración\n",
    "       to_ix: diccionario de palabras.\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de frases con errores:  5147\n",
      "Numero de frases sin errores:  71\n"
     ]
    }
   ],
   "source": [
    "#limpieza de datos \n",
    "# Read the data from the CSV file\n",
    "labels = [\"DET\", \"NN\", \"V\", \"ADJ\", \"PREP\"]\n",
    "with open('FULL DATA.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "with open('FULL DATA.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "counter = 0\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las frases con otra dificultad\n",
    "for i in data:\n",
    "    if len(i) > 8 or len(i) < 8:\n",
    "        counter += 1\n",
    "        list_to_delete.append(i)\n",
    "\n",
    "for i in list_to_delete:\n",
    "    data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las frases con diferete logitud\n",
    "for i in data:\n",
    "    sentence = i[0].split()\n",
    "    if len(sentence) > 7 or len(sentence) < 7:\n",
    "        counter += 1\n",
    "        list_to_delete.append(i)\n",
    "\n",
    "for i in list_to_delete:\n",
    "    data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las frases con etiquetas no reconocidas\n",
    "for i in data:\n",
    "    for j in range(len(i)):\n",
    "        if j > 0 and i[j] not in labels:\n",
    "            counter += 1\n",
    "            list_to_delete.append(i)\n",
    "\n",
    "\n",
    "for i in list_to_delete:\n",
    "    if i in data:\n",
    "        data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "print(\"Numero de frases con errores: \", counter)\n",
    "print(\"Numero de frases sin errores: \", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "[[['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardin'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'mesa', 'grande', 'esta', 'en', 'la', 'cocina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'perro', 'marron', 'ladra', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'libro', 'interesante', 'esta', 'en', 'la', 'biblioteca'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'ninia', 'pequenia', 'juega', 'en', 'el', 'patio'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pelota', 'roja', 'rueda', 'en', 'el', 'suelo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'hombre', 'mayor', 'camina', 'por', 'la', 'calle'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'mujer', 'guapa', 'esta', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ninio', 'feliz', 'juega', 'en', 'la', 'playa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'coche', 'rapido', 'pasa', 'por', 'la', 'carretera'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'rio', 'caudaloso', 'fluye', 'hacia', 'el', 'mar'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ave', 'libre', 'vuela', 'por', 'el', 'cielo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'flor', 'hermosa', 'esta', 'en', 'el', 'jardin'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ruido', 'fuerte', 'asusta', 'a', 'los', 'pajaros'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'viento', 'fresco', 'sopla', 'entre', 'los', 'arboles'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ninio', 'travieso', 'juega', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'agua', 'clara', 'fluye', 'por', 'el', 'rio'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'sol', 'radiante', 'brilla', 'en', 'el', 'horizonte'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'pajaro', 'colorido', 'canta', 'en', 'la', 'rama'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pintura', 'abstracta', 'adorna', 'la', 'pared', 'blanca'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'vaso', 'vacio', 'esta', 'sobre', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'toalla', 'mojada', 'cuelga', 'en', 'el', 'banio'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'maestro', 'sabio', 'ensenia', 'a', 'sus', 'alumnos'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'silencio', 'absoluto', 'reina', 'en', 'la', 'noche'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'lluvia', 'fina', 'cae', 'sobre', 'el', 'suelo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'calor', 'intenso', 'agobia', 'a', 'la', 'gente'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ninio', 'inquieto', 'corre', 'por', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'ninia', 'creativa', 'dibuja', 'en', 'su', 'cuaderno'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ruido', 'estruendoso', 'asusta', 'a', 'los', 'animales'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'vaso', 'lleno', 'esta', 'sobre', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'maestro', 'paciente', 'guia', 'a', 'sus', 'estudiantes'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'grande', 'esta', 'en', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'gato', 'negro', 'duerme', 'en', 'el', 'sofa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'libro', 'interesante', 'esta', 'en', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'ninia', 'feliz', 'juega', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pelota', 'roja', 'rueda', 'en', 'el', 'pasto'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'blanca', 'esta', 'en', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'gato', 'blanco', 'duerme', 'en', 'la', 'cama'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'libro', 'aburrido', 'esta', 'en', 'el', 'estante'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'ninia', 'pequenia', 'juega', 'en', 'el', 'jardin'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pelota', 'amarilla', 'rueda', 'en', 'el', 'suelo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'perro', 'blanco', 'corre', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'moderna', 'esta', 'en', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'gato', 'naranja', 'duerme', 'en', 'la', 'silla'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'libro', 'viejo', 'esta', 'en', 'la', 'biblioteca'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pelota', 'azul', 'rueda', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'perro', 'marron', 'corre', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'antigua', 'esta', 'en', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'gato', 'gris', 'duerme', 'en', 'el', 'techo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'blanca', 'esta', 'en', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'carro', 'rapido', 'corre', 'por', 'la', 'carretera'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ninio', 'feliz', 'juega', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'bebe', 'hermoso', 'duerme', 'en', 'la', 'cuna'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'reloj', 'antiguo', 'esta', 'en', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'taza', 'vacia', 'esta', 'en', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'zapato', 'comodo', 'es', 'ideal', 'para', 'caminar'], ['DET', 'NN', 'ADJ', 'V', 'ADJ', 'PREP', 'NN']]]\n",
      "Test data:\n",
      "[[['La', 'maleta', 'pesada', 'es', 'dificil', 'de', 'llevar'], ['DET', 'NN', 'ADJ', 'V', 'ADJ', 'PREP', 'NN']], [['El', 'pollo', 'crujiente', 'es', 'mi', 'plato', 'favorito'], ['DET', 'NN', 'ADJ', 'V', 'ADJ', 'DET', 'NN']], [['La', 'puerta', 'cerrada', 'no', 'permite', 'el', 'paso'], ['DET', 'NN', 'ADJ', 'V', 'ADJ', 'DET', 'NN']], [['El', 'cuaderno', 'nuevo', 'esta', 'en', 'la', 'mochila'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'barco', 'grande', 'navega', 'por', 'el', 'oceano'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'mesa', 'redonda', 'esta', 'en', 'el', 'comedor'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'casa', 'azul', 'esta', 'en', 'la', 'esquina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'auto', 'rojo', 'corre', 'por', 'la', 'carretera'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'nube', 'blanca', 'flota', 'en', 'el', 'cielo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'rio', 'caudaloso', 'fluye', 'hacia', 'el', 'mar'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'piedra', 'redonda', 'rueda', 'por', 'la', 'colina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'botella', 'vacia', 'esta', 'en', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ninio', 'feliz', 'corre', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'avion', 'grande', 'vuela', 'hacia', 'el', 'sur'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'pez', 'dorado', 'nada', 'en', 'el', 'acuario'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']]]\n",
      "Diccionario {'El': 0, 'gato': 1, 'negro': 2, 'corre': 3, 'en': 4, 'el': 5, 'jardin': 6, 'La': 7, 'mesa': 8, 'grande': 9, 'esta': 10, 'la': 11, 'cocina': 12, 'perro': 13, 'marron': 14, 'ladra': 15, 'parque': 16, 'libro': 17, 'interesante': 18, 'biblioteca': 19, 'ninia': 20, 'pequenia': 21, 'juega': 22, 'patio': 23, 'pelota': 24, 'roja': 25, 'rueda': 26, 'suelo': 27, 'hombre': 28, 'mayor': 29, 'camina': 30, 'por': 31, 'calle': 32, 'mujer': 33, 'guapa': 34, 'ninio': 35, 'feliz': 36, 'playa': 37, 'coche': 38, 'rapido': 39, 'pasa': 40, 'carretera': 41, 'rio': 42, 'caudaloso': 43, 'fluye': 44, 'hacia': 45, 'mar': 46, 'ave': 47, 'libre': 48, 'vuela': 49, 'cielo': 50, 'flor': 51, 'hermosa': 52, 'ruido': 53, 'fuerte': 54, 'asusta': 55, 'a': 56, 'los': 57, 'pajaros': 58, 'viento': 59, 'fresco': 60, 'sopla': 61, 'entre': 62, 'arboles': 63, 'travieso': 64, 'agua': 65, 'clara': 66, 'sol': 67, 'radiante': 68, 'brilla': 69, 'horizonte': 70, 'pajaro': 71, 'colorido': 72, 'canta': 73, 'rama': 74, 'pintura': 75, 'abstracta': 76, 'adorna': 77, 'pared': 78, 'blanca': 79, 'vaso': 80, 'vacio': 81, 'sobre': 82, 'toalla': 83, 'mojada': 84, 'cuelga': 85, 'banio': 86, 'maestro': 87, 'sabio': 88, 'ensenia': 89, 'sus': 90, 'alumnos': 91, 'silencio': 92, 'absoluto': 93, 'reina': 94, 'noche': 95, 'lluvia': 96, 'fina': 97, 'cae': 98, 'calor': 99, 'intenso': 100, 'agobia': 101, 'gente': 102, 'inquieto': 103, 'creativa': 104, 'dibuja': 105, 'su': 106, 'cuaderno': 107, 'estruendoso': 108, 'animales': 109, 'lleno': 110, 'paciente': 111, 'guia': 112, 'estudiantes': 113, 'casa': 114, 'colina': 115, 'duerme': 116, 'sofa': 117, 'pasto': 118, 'blanco': 119, 'cama': 120, 'aburrido': 121, 'estante': 122, 'amarilla': 123, 'moderna': 124, 'naranja': 125, 'silla': 126, 'viejo': 127, 'azul': 128, 'antigua': 129, 'gris': 130, 'techo': 131, 'carro': 132, 'bebe': 133, 'hermoso': 134, 'cuna': 135, 'reloj': 136, 'antiguo': 137, 'taza': 138, 'vacia': 139, 'zapato': 140, 'comodo': 141, 'es': 142, 'ideal': 143, 'para': 144, 'caminar': 145, 'maleta': 146, 'pesada': 147, 'dificil': 148, 'de': 149, 'llevar': 150, 'pollo': 151, 'crujiente': 152, 'mi': 153, 'plato': 154, 'favorito': 155, 'puerta': 156, 'cerrada': 157, 'no': 158, 'permite': 159, 'paso': 160, 'nuevo': 161, 'mochila': 162, 'barco': 163, 'navega': 164, 'oceano': 165, 'redonda': 166, 'comedor': 167, 'esquina': 168, 'auto': 169, 'rojo': 170, 'nube': 171, 'flota': 172, 'piedra': 173, 'botella': 174, 'avion': 175, 'sur': 176, 'pez': 177, 'dorado': 178, 'nada': 179, 'acuario': 180}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cantidad de etiquetas encontradas en el texto:\n",
      "DET  :  140\n",
      "NN  :  142\n",
      "V  :  71\n",
      "ADJ  :  75\n",
      "PREP  :  69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Divide the data into training and testing sets\n",
    "#random.shuffle(data)\n",
    "split = int(0.8 * len(data))\n",
    "training_data = data[:split]\n",
    "test_data = data[split:]\n",
    "\n",
    "\n",
    "# Split the training data into text and labels\n",
    "for i in range(len(training_data)):\n",
    "    splitted_text = training_data[i][0].split()\n",
    "    #put the rest of i in other list\n",
    "    labels = []\n",
    "    for j in range(len(training_data[i])):\n",
    "        if j != 0:\n",
    "            labels.append(training_data[i][j])\n",
    "    training_data[i] = [0,0]\n",
    "    training_data[i][0] = splitted_text\n",
    "    training_data[i][1] = labels\n",
    "\n",
    "# Split the test data into text and labels\n",
    "for i in range(len(test_data)):\n",
    "    splitted_text = test_data[i][0].split()\n",
    "    #put the rest of i in other list\n",
    "    labels = []\n",
    "    for j in range(len(test_data[i])):\n",
    "        if j != 0:\n",
    "            labels.append(test_data[i][j])\n",
    "    test_data[i] = [0,0]\n",
    "    test_data[i][0] = splitted_text\n",
    "    test_data[i][1] = labels\n",
    "\n",
    "# print the data\n",
    "print(\"Training data:\")\n",
    "print(training_data)\n",
    "print(\"Test data:\")\n",
    "print(test_data)\n",
    "\n",
    "\n",
    "# Diccionario las palabras\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(\"Diccionario\", word_to_ix)\n",
    "\n",
    "# Asignar índices a las etiquetas\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\":3, \"PREP\":4}\n",
    "\n",
    "#print the amount of each tag found in the text, print that in a table using pandas\n",
    "\n",
    "print(\"\\n\\n\\n\\nCantidad de etiquetas encontradas en el texto:\")\n",
    "for tag in tag_to_ix:\n",
    "    count = 0\n",
    "    for sent, tags in training_data + test_data:\n",
    "        for word in tags:\n",
    "            if word == tag:\n",
    "                count += 1\n",
    "    print(tag,\" : \", count)\n",
    "\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    #put the rest of i in other list\n",
    "    if len(training_data[i][1]) == 6:\n",
    "        print(training_data[i])\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    #put the rest of i in other list\n",
    "    if len(test_data[i][1]) == 6:\n",
    "        print(test_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardin']\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de procesamiento de una oración\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(training_data[0][0])                          \n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLmaT08_DfVm"
   },
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Clase para aplicar POST a oraciones en español. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Incialización del modelo\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        \"\"\"\n",
    "        Inicialización de la clase.\n",
    "        Parámetros:\n",
    "           embedding_dim: dimesionalidad del vector de palabras. \n",
    "           hidden_dim: dimensión de la capa oculta de la red. \n",
    "           vocab_size: tamaño del vocabulario.  \n",
    "           tagset_size: número de clases.\n",
    "        \"\"\"\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Primero se pasa la entrada a través de una capa Embedding. \n",
    "        # Esta capa construye una representación de los tokens de \n",
    "        # un texto donde las palabras que tienen el mismo significado \n",
    "        # tienen una representación similar.\n",
    "        \n",
    "        # Esta capa captura mejor el contexto y son espacialmente \n",
    "        # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "        # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "        # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "        # de palabras como entrada. Ejemplos en [3] y [4]\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # El LSTM toma word_embeddings como entrada y genera los estados ocultos\n",
    "        # con dimensionalidad hidden_dim.  \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # La capa lineal mapea el espacio de estado oculto \n",
    "        # al espacio de clases\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pasada hacia adelante de la red. \n",
    "        # Parámetros:\n",
    "        #    sentence: la oración a procesar\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "\n",
    "        # Se utiliza softmax para devolver la probabilidad de cada etiqueta\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHhOk08h1qY2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "#Nodos se subieron a 10 con la intencion para experimentar\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 10\n",
    "\n",
    "\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Generalmente utilizada en problemas de clasificacion con múltiples clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "bRYCyKy-DfVu",
    "outputId": "5ccdff4d-dcca-4545-81bf-69a396ec4fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardin']\n",
      "tensor([[-1.6499, -1.6724, -1.4370, -1.8541, -1.4876],\n",
      "        [-1.4980, -1.7850, -1.4924, -2.0177, -1.3830],\n",
      "        [-1.4917, -1.8383, -1.4603, -1.9947, -1.3955],\n",
      "        [-1.4995, -1.7520, -1.5521, -1.9660, -1.3803],\n",
      "        [-1.5165, -1.7765, -1.5060, -2.0250, -1.3566],\n",
      "        [-1.5668, -1.7597, -1.4809, -2.0213, -1.3499],\n",
      "        [-1.5467, -1.8426, -1.4104, -1.9674, -1.4073]])\n",
      "Resultados luego del entrenamiento para la primera frase\n",
      "tensor([[-3.1145e-04, -8.2100e+00, -1.4919e+01, -1.2563e+01, -1.0242e+01],\n",
      "        [-8.1076e+00, -1.3618e-03, -1.3716e+01, -6.8509e+00, -1.5676e+01],\n",
      "        [-1.2281e+01, -8.1560e+00, -6.9182e+00, -1.2858e-03, -1.2522e+01],\n",
      "        [-1.3795e+01, -1.2786e+01, -1.9536e-03, -6.4533e+00, -7.8954e+00],\n",
      "        [-9.7351e+00, -1.5004e+01, -7.7160e+00, -1.1209e+01, -5.1878e-04],\n",
      "        [-8.5758e-04, -7.6208e+00, -1.4294e+01, -1.3064e+01, -7.9175e+00],\n",
      "        [-8.0319e+00, -4.5718e-04, -1.5568e+01, -8.9380e+00, -1.4190e+01]])\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    print(training_data[0][0])\n",
    "    \n",
    "    # Clasificación    \n",
    "    print(tag_scores)\n",
    "\n",
    "acc =0\n",
    "# Épocas de entrenamiento\n",
    "for epoch in range(200):  \n",
    "    for sentence, tags in training_data:\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    print(tag_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score for training data:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "def get_precision_score(model, data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for sentence, tags in data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        tag_scores = model(sentence_in)\n",
    "        for i in range(len(tag_scores)):\n",
    "            total += 1\n",
    "            if torch.argmax(tag_scores[i]) == targets[i]:\n",
    "                correct += 1\n",
    "    return correct/total\n",
    "\n",
    "print(\"Precision score for training data: \", get_precision_score(model, training_data)*100, \"%\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exhaustividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_score(y_test, y_pred, average):\n",
    "    if average == \"macro\":\n",
    "        recall = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == y_pred[i]:\n",
    "                recall += 1\n",
    "        return recall/len(y_test)\n",
    "    elif average == \"micro\":\n",
    "        recall = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == y_pred[i]:\n",
    "                recall += 1\n",
    "        return recall/len(y_test)\n",
    "    elif average == \"weighted\":\n",
    "        recall = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == y_pred[i]:\n",
    "                recall += 1\n",
    "        return recall/len(y_test)\n",
    "    else:\n",
    "        print(\"Invalid average parameter\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  97.05882352941177 %\n"
     ]
    }
   ],
   "source": [
    "def f1_score(y_true, y_pred, average='macro'):\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = get_precision_score(model, training_data)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "y_pred = []\n",
    "for sentence, tags in test_data:\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    targets = prepare_sequence(tags, tag_to_ix)\n",
    "    tag_scores = model(sentence_in)\n",
    "    for i in range(len(tag_scores)):\n",
    "        y_pred.append(torch.argmax(tag_scores[i]))\n",
    "\n",
    "\n",
    "y_test = []\n",
    "for sentence, tags in test_data:\n",
    "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "    targets = prepare_sequence(tags, tag_to_ix)\n",
    "    for i in range(len(targets)):\n",
    "        y_test.append(targets[i])\n",
    "        \n",
    "f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1 score: \", f1_score*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "sPj6v4tsDfV2",
    "outputId": "6e5d5197-c82f-4d22-c74c-cb22c5b20af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases\n",
      "{'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n",
      "FRASE\n",
      "La frase original ['La', 'maleta', 'pesada', 'es', 'dificil', 'de', 'llevar']\n",
      "La frase original preprocesada tensor([  7, 146, 147, 142, 148, 149, 150])\n",
      "Salida del modelo tensor([[-1.3838e-03, -6.6321e+00, -1.1651e+01, -1.0529e+01, -1.0415e+01],\n",
      "        [-7.2527e+00, -1.9346e-03, -1.2886e+01, -6.7082e+00, -1.3741e+01],\n",
      "        [-1.2640e+01, -6.2074e+00, -5.6051e+00, -5.7328e-03, -1.0837e+01],\n",
      "        [-1.0337e+01, -1.0571e+01, -3.2290e-02, -4.9976e+00, -3.6904e+00],\n",
      "        [-5.6293e+00, -8.9339e+00, -2.5343e+00, -4.8255e+00, -9.5480e-02],\n",
      "        [-1.2010e-01, -3.1737e+00, -9.4935e+00, -9.1159e+00, -2.6432e+00],\n",
      "        [-1.1699e+01, -4.2418e-04, -1.2586e+01, -7.7982e+00, -1.3259e+01]])\n",
      "Valores máximos e índices (tensor([-0.0014, -0.0019, -0.0057, -0.0323, -0.0955, -0.1201, -0.0004]), tensor([0, 1, 3, 2, 4, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Uso del modelo generado\n",
    "def test_examples(test_data):\n",
    "\n",
    "   with torch.no_grad():\n",
    "      inputs = prepare_sequence(test_data, word_to_ix)\n",
    "      tag_scores = model(inputs)\n",
    "    \n",
    " \n",
    "   print(\"FRASE\") \n",
    "   print(\"La frase original\", test_data)    \n",
    "   print(\"La frase original preprocesada\", inputs)\n",
    "   print(\"Salida del modelo\", tag_scores)\n",
    "   print(\"Valores máximos e índices\", max_values(tag_scores))    \n",
    "    \n",
    "print(\"Clases\")\n",
    "print(tag_to_ix)\n",
    "\n",
    "#Frase 1\n",
    "test_examples(test_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASE\n",
      "La frase original ['El', 'cuaderno', 'nuevo', 'esta', 'en', 'la', 'mochila']\n",
      "La frase original preprocesada tensor([  0, 107, 161,  10,   4,  11, 162])\n",
      "Salida del modelo tensor([[-3.1145e-04, -8.2100e+00, -1.4919e+01, -1.2563e+01, -1.0242e+01],\n",
      "        [-9.7018e+00, -3.7472e-04, -1.3896e+01, -8.0716e+00, -1.5072e+01],\n",
      "        [-1.1576e+01, -6.6076e+00, -6.9404e+00, -2.3400e-03, -1.1531e+01],\n",
      "        [-1.4923e+01, -1.3253e+01, -1.1129e-03, -7.1394e+00, -8.0569e+00],\n",
      "        [-9.8896e+00, -1.5076e+01, -7.7739e+00, -1.1367e+01, -4.8328e-04],\n",
      "        [-6.8558e-04, -7.7612e+00, -1.4287e+01, -1.3421e+01, -8.2650e+00],\n",
      "        [-6.4974e+00, -2.2295e-03, -1.2840e+01, -7.2442e+00, -1.2805e+01]])\n",
      "Valores máximos e índices (tensor([-0.0003, -0.0004, -0.0023, -0.0011, -0.0005, -0.0007, -0.0022]), tensor([0, 1, 3, 2, 4, 0, 1]))\n",
      "valor de las etiquetas {'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n"
     ]
    }
   ],
   "source": [
    "#Frase 2\n",
    "test_examples(test_data[3][0])\n",
    "\n",
    "print(\"valor de las etiquetas\", tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASE\n",
      "La frase original ['La', 'puerta', 'cerrada', 'no', 'permite', 'el', 'paso']\n",
      "La frase original preprocesada tensor([  7, 156, 157, 158, 159,   5, 160])\n",
      "Salida del modelo tensor([[-1.3838e-03, -6.6321e+00, -1.1651e+01, -1.0529e+01, -1.0415e+01],\n",
      "        [-6.9696e+00, -3.6249e-03, -1.2117e+01, -5.9252e+00, -1.3306e+01],\n",
      "        [-1.2493e+01, -7.0096e+00, -6.9012e+00, -1.9216e-03, -1.1991e+01],\n",
      "        [-8.7841e+00, -1.1287e+01, -1.9371e-02, -4.0130e+00, -6.9710e+00],\n",
      "        [-5.2524e+00, -1.1763e+01, -3.1264e+00, -6.2656e+00, -5.2367e-02],\n",
      "        [-1.5425e-03, -6.8375e+00, -1.3948e+01, -1.2726e+01, -7.6741e+00],\n",
      "        [-8.5009e+00, -5.8479e-04, -1.3878e+01, -7.8750e+00, -1.5050e+01]])\n",
      "Valores máximos e índices (tensor([-0.0014, -0.0036, -0.0019, -0.0194, -0.0524, -0.0015, -0.0006]), tensor([0, 1, 3, 2, 4, 0, 1]))\n",
      "valor de las etiquetas {'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n"
     ]
    }
   ],
   "source": [
    "# Otra prueba\n",
    "test_examples(test_data[2][0])\n",
    "print(\"valor de las etiquetas\", tag_to_ix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "1. En el caso de LSTM consideramos que es un tipo de red neuronal recurrente especialmente eficaz para procesar datos secuenciales. EL punto fuerte de LSTM es que puede recordar información del pasado y utilizarla para realizar predicciones en el futuro. Esto hace que las LSTM sean especialmente útiles para tareas como el procesamiento del lenguaje natural, el reconocimiento del habla, el análisis de vídeo, entre otros.\n",
    "2. Nos dimos cuenta que el LSTM no solo se puede aplicar al NLP, también tiene otras aplicaciones en las que por su memoria y capacidad de recordar el pasado para aplicarlo al futuro se convierte útil en aplicaciones en áreas como las finanzas, la medicina, la robótica y la composición musical. Por ejemplo, investigando un poco sobre LSTM encontramos que en finanzas, LSTM se ha utilizado para predecir los precios de las acciones, mientras que en medicina se ha empleado para el diagnóstico de enfermedades y el descubrimiento de fármacos. Por lo que LSTM no solo se limita al NLP y eso lo hace muy valioso.\n",
    "3. También consideramos de vital importancia el tema del NLP el cuál no se debe minimizar ni subestimar ya que es uno subcampos que ha evolucionado rápidamente en los últimos años debido a los avances en las técnicas de aprendizaje automático, la disponibilidad de grandes cantidades de datos de texto y el desarrollo de recursos informáticos más potentes. El mismo subcampo del NLP tiene una amplia gama de aplicaciones, desde los chatbots y los asistentes virtuales hasta el análisis de sentimientos, la traducción automática y la extracción de información.\n",
    "4. Nos parecio muy interesante el tema del POS y como este define mas o menos las instrucciones para la red neuronal, creemos muy importante conocer eso sin embargo nos llama mucho la atención el como podría funcionar una red neuronal no supervisada la cúal casi que no tenga parametros o formas de saber como dividir los datos, solo se le provean resultados. Sin emabrgo la precisión de estos algoritmos varía en función de la complejidad y ambigüedad de la lengua analizada, en el caso del español se tuvo que \"normalizar\" los datos para que no hubieran demasiados problemas.\n",
    "\n",
    "\n",
    "\n",
    "PD: Profe, no estamos muy seguros de si funciona del todo bien el tema de precision, exhaustividad y F1 entonces por razones de tiempo lo entregamos así, sin embargo seguiremos investigando al respecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZjYZ9vfDfV9"
   },
   "source": [
    "# Referencias \n",
    "\n",
    "[1] Guthrie, R. (2017). Tutorial. Sequence Models and Long-Short Term Memory Networks. Recuperado de https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "[2] LeCun,Y., Bengio, Y.,  & Hinton, G. (2015). Deep learning. Nature, 521(7553):436.\n",
    "\n",
    "[3] Brownlee, J. (2017). What Are Word Embeddings for Text?. Recuperado de https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "[4] Bishop, C (2006). Pattern Recognition and Machine Learning. Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "16_POST_with_LSTM_(Esp).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
