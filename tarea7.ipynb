{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehUxc7AtDfU5"
   },
   "source": [
    "### ITCR - Escuela de Computación\n",
    "### Curso IC-6200 - Inteligencia Artificial\n",
    "### Aprendizaje supervisado\n",
    "\n",
    "### Redes de memoria de corto y largo plazo con PyTorch \n",
    "### (Long-Short Term Memory Networks-LSTM)\n",
    "\n",
    "**Profesora: María Auxiliadora Mora**\n",
    "\n",
    "### Tarea corta #7\n",
    "\n",
    "### Estudiantes:\n",
    "**1. Pablo Alberto Muñoz Hidalgo**\n",
    "\n",
    "**2. Luis Andrés Rojas Murillo**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyr84u8ab1kw"
   },
   "source": [
    "## Introducción\n",
    "\n",
    "La clasificación de textos y el reconocimiento de entidades nombradas (Named Entity Recognition o NER por sus siglas en inglés) son técnicas fundamentales que constituyen el primer paso en muchas tareas de Procesamiento de lenguaje natural (NLP). NER, es un área de investigación relacionada a la extracción de información, que permite localizar y clasificar nombres de entidades que se encuentran en texto libre, en categorías comunmente organizaciones, lugares, tiempo, personas, entre otros. Ejemplo:\n",
    "\n",
    "- El fundador de [Microsoft Corporation] (organización), [Bill Gates] (persona), comentó que se abrirán 1000 puestos de trabajo en la [Región Chorotega] (lugar) a partir del año 2022 (fecha).  \n",
    "\n",
    "La clasificación de textos permite categorizar el contenido asociando este a un conjunto de etiquetas predefinidas o clases. Su uso más popular es el análisis de sentimientos. Ejemplo:\n",
    "\n",
    "- En mi opinión, la película fue muy buena porque pudo dar a conocer a los espectadores cómo puede afectar una situación traumática a la mente humana. (Clase = 5 o excelente). \n",
    "\n",
    "Las redes neuronales recurrentes o RNN (Rumelhart et al., 1986, como se citó en LeCun et al., 2015) son una familia de redes neuronales para el procesamiento de secuencias de datos, las cuales en un tiempo t, reciben el estado anterior, es decir, su salida en el tiempo t podría usarse como insumo del procesamiento de la siguiente entrada, de modo que la información pueda propagarse a medida que la red pasa por la secuencia de entrada. Las redes Long Short-Term Memory (LSTM) son un tipo de red neuronal recurrente capaz de aprender dependencias a largo plazo.\n",
    "\n",
    "El siguiente ejemplo implementa NER con una LSTM para etiquetar el rol que juegan las palabras en las oraciones. \n",
    "\n",
    "\n",
    "## Ejemplo\n",
    "\n",
    "El sistema implementado en el código adjunto soluciona el problema de estimar el rol de una palabra en una frase, por ejempo roles como determinante (DET), nombre (NN) y verbo (V). \n",
    "Ejemplo para la frase:\n",
    "\n",
    "- \"El perro come manzana\" la salida deberá ser: [\"DET\", \"NN\", \"V\", \"NN\"]). \n",
    "\n",
    "Este proceso se conoce en el procesamiento de lenguaje natural como \"part of speech tagging (POS)\".\n",
    "\n",
    "Este es un ejemplo simple con datos introducidos en el código basado en [1].\n",
    "\n",
    "Se realizarán los siguientes pasos\n",
    "\n",
    "   * Definición de los ejemplos (codificados) \n",
    "   * Preprocesamiento de las palabras a clasificar\n",
    "   * Definición del modelo\n",
    "   * Instanciación del modelo, definición de la función de pérdida y del optimizador  \n",
    "   * Entrenamiento de la red\n",
    "   * Pruebas del modelo resultante con unos cuantos ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gmqx6kMSDfU9",
    "outputId": "c6a51d34-4aeb-4cd4-ed22-b92d297ea46e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x507b450>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bibliotecas requeridas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import random\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsFmju8xDfVQ"
   },
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "def max_values(x):\n",
    "    \"\"\"\n",
    "    Retorna el valor máximo y en índice o la posición del valor en un vector x.\n",
    "    Parámetros: \n",
    "        x: vector con los datos. \n",
    "    Salida: \n",
    "        out: valor \n",
    "        inds: índice\n",
    "    \"\"\"\n",
    "    out, inds = torch.max(x,dim=1)   \n",
    "    return out, inds\n",
    "    \n",
    "\n",
    "    # Preparación de los datos \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"\n",
    "    Retorna un tensor con los indices del diccionario para cada palabras en una oración.\n",
    "    Parámetros:\n",
    "       seq: oración\n",
    "       to_ix: diccionario de palabras.\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m             counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     40\u001b[0m             list_to_delete\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m---> 43\u001b[0m data\u001b[39m.\u001b[39;49mremove(list_to_delete[:\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     44\u001b[0m list_to_delete \u001b[39m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumero de frases con errores: \u001b[39m\u001b[39m\"\u001b[39m, counter)\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "#limpieza de datos \n",
    "# Read the data from the CSV file\n",
    "labels = [\"DET\", \"NN\", \"V\", \"ADJ\", \"PREP\"]\n",
    "with open('FULL DATA.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "with open('FULL DATA.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "counter = 0\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las fraces con otra dificultad\n",
    "for i in data:\n",
    "    if len(i) > 8 or len(i) < 8:\n",
    "        counter += 1\n",
    "        list_to_delete.append(i)\n",
    "\n",
    "for i in list_to_delete:\n",
    "    data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las fraces con diferete logitud\n",
    "for i in data:\n",
    "    sentence = i[0].split()\n",
    "    if len(sentence) > 7 or len(sentence) < 7:\n",
    "        counter += 1\n",
    "        list_to_delete.append(i)\n",
    "\n",
    "for i in list_to_delete:\n",
    "    data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "#todas las fraces con etiquetas no reconocidas\n",
    "for i in data:\n",
    "    for j in range(len(i)):\n",
    "        if j > 0 and i[j] not in labels:\n",
    "            counter += 1\n",
    "            list_to_delete.append(i)\n",
    "\n",
    "\n",
    "for i in list_to_delete:\n",
    "    if i in data:\n",
    "        data.remove(i)\n",
    "list_to_delete = []\n",
    "\n",
    "print(\"Numero de frases con errores: \", counter)\n",
    "print(\"Numero de frases sin errores: \", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "[[['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardÃ\\xadn'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'mesa', 'grande', 'estÃ¡', 'en', 'la', 'cocina'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'perro', 'marrÃ³n', 'ladra', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'libro', 'interesante', 'estÃ¡', 'en', 'la', 'biblioteca'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'niÃ±a', 'pequeÃ±a', 'juega', 'en', 'el', 'patio'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pelota', 'roja', 'rueda', 'en', 'el', 'suelo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'hombre', 'mayor', 'camina', 'por', 'la', 'calle'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'mujer', 'guapa', 'estÃ¡', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'niÃ±o', 'feliz', 'juega', 'en', 'la', 'playa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'coche', 'rÃ¡pido', 'pasa', 'por', 'la', 'carretera'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'rÃ\\xado', 'caudaloso', 'fluye', 'hacia', 'el', 'mar'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ave', 'libre', 'vuela', 'por', 'el', 'cielo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'flor', 'hermosa', 'estÃ¡', 'en', 'el', 'jardÃ\\xadn'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ruido', 'fuerte', 'asusta', 'a', 'los', 'pÃ¡jaros'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'viento', 'fresco', 'sopla', 'entre', 'los', 'Ã¡rboles'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'niÃ±o', 'travieso', 'juega', 'en', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'agua', 'clara', 'fluye', 'por', 'el', 'rÃ\\xado'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'sol', 'radiante', 'brilla', 'en', 'el', 'horizonte'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'pÃ¡jaro', 'colorido', 'canta', 'en', 'la', 'rama'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'pintura', 'abstracta', 'adorna', 'la', 'pared', 'blanca'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'vaso', 'vacÃ\\xado', 'estÃ¡', 'sobre', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'toalla', 'mojada', 'cuelga', 'en', 'el', 'baÃ±o'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'maestro', 'sabio', 'enseÃ±a', 'a', 'sus', 'alumnos'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'silencio', 'absoluto', 'reina', 'en', 'la', 'noche'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']]]\n",
      "Test data:\n",
      "[[['La', 'lluvia', 'fina', 'cae', 'sobre', 'el', 'suelo'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'calor', 'intenso', 'agobia', 'a', 'la', 'gente'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'niÃ±o', 'inquieto', 'corre', 'por', 'el', 'parque'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['La', 'niÃ±a', 'creativa', 'dibuja', 'en', 'su', 'cuaderno'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'ruido', 'estruendoso', 'asusta', 'a', 'los', 'animales'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'vaso', 'lleno', 'estÃ¡', 'sobre', 'la', 'mesa'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']], [['El', 'maestro', 'paciente', 'guÃ\\xada', 'a', 'sus', 'estudiantes'], ['DET', 'NN', 'ADJ', 'V', 'PREP', 'DET', 'NN']]]\n",
      "Diccionario {'El': 0, 'gato': 1, 'negro': 2, 'corre': 3, 'en': 4, 'el': 5, 'jardÃ\\xadn': 6, 'La': 7, 'mesa': 8, 'grande': 9, 'estÃ¡': 10, 'la': 11, 'cocina': 12, 'perro': 13, 'marrÃ³n': 14, 'ladra': 15, 'parque': 16, 'libro': 17, 'interesante': 18, 'biblioteca': 19, 'niÃ±a': 20, 'pequeÃ±a': 21, 'juega': 22, 'patio': 23, 'pelota': 24, 'roja': 25, 'rueda': 26, 'suelo': 27, 'hombre': 28, 'mayor': 29, 'camina': 30, 'por': 31, 'calle': 32, 'mujer': 33, 'guapa': 34, 'niÃ±o': 35, 'feliz': 36, 'playa': 37, 'coche': 38, 'rÃ¡pido': 39, 'pasa': 40, 'carretera': 41, 'rÃ\\xado': 42, 'caudaloso': 43, 'fluye': 44, 'hacia': 45, 'mar': 46, 'ave': 47, 'libre': 48, 'vuela': 49, 'cielo': 50, 'flor': 51, 'hermosa': 52, 'ruido': 53, 'fuerte': 54, 'asusta': 55, 'a': 56, 'los': 57, 'pÃ¡jaros': 58, 'viento': 59, 'fresco': 60, 'sopla': 61, 'entre': 62, 'Ã¡rboles': 63, 'travieso': 64, 'agua': 65, 'clara': 66, 'sol': 67, 'radiante': 68, 'brilla': 69, 'horizonte': 70, 'pÃ¡jaro': 71, 'colorido': 72, 'canta': 73, 'rama': 74, 'pintura': 75, 'abstracta': 76, 'adorna': 77, 'pared': 78, 'blanca': 79, 'vaso': 80, 'vacÃ\\xado': 81, 'sobre': 82, 'toalla': 83, 'mojada': 84, 'cuelga': 85, 'baÃ±o': 86, 'maestro': 87, 'sabio': 88, 'enseÃ±a': 89, 'sus': 90, 'alumnos': 91, 'silencio': 92, 'absoluto': 93, 'reina': 94, 'noche': 95, 'lluvia': 96, 'fina': 97, 'cae': 98, 'calor': 99, 'intenso': 100, 'agobia': 101, 'gente': 102, 'inquieto': 103, 'creativa': 104, 'dibuja': 105, 'su': 106, 'cuaderno': 107, 'estruendoso': 108, 'animales': 109, 'lleno': 110, 'paciente': 111, 'guÃ\\xada': 112, 'estudiantes': 113}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cantidad de etiquetas encontradas en el texto:\n",
      "DET  :  62\n",
      "NN  :  62\n",
      "V  :  31\n",
      "ADJ  :  31\n",
      "PREP  :  31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Divide the data into training and testing sets\n",
    "#random.shuffle(data)\n",
    "split = int(0.8 * len(data))\n",
    "training_data = data[:split]\n",
    "test_data = data[split:]\n",
    "\n",
    "\n",
    "# Split the training data into text and labels\n",
    "for i in range(len(training_data)):\n",
    "    splitted_text = training_data[i][0].split()\n",
    "    #put the rest of i in other list\n",
    "    labels = []\n",
    "    for j in range(len(training_data[i])):\n",
    "        if j != 0:\n",
    "            labels.append(training_data[i][j])\n",
    "    training_data[i] = [0,0]\n",
    "    training_data[i][0] = splitted_text\n",
    "    training_data[i][1] = labels\n",
    "\n",
    "# Split the test data into text and labels\n",
    "for i in range(len(test_data)):\n",
    "    splitted_text = test_data[i][0].split()\n",
    "    #put the rest of i in other list\n",
    "    labels = []\n",
    "    for j in range(len(test_data[i])):\n",
    "        if j != 0:\n",
    "            labels.append(test_data[i][j])\n",
    "    test_data[i] = [0,0]\n",
    "    test_data[i][0] = splitted_text\n",
    "    test_data[i][1] = labels\n",
    "\n",
    "# print the data\n",
    "print(\"Training data:\")\n",
    "print(training_data)\n",
    "print(\"Test data:\")\n",
    "print(test_data)\n",
    "\n",
    "\n",
    "# Diccionario las palabras\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(\"Diccionario\", word_to_ix)\n",
    "\n",
    "# Asignar índices a las etiquetas\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\":3, \"PREP\":4}\n",
    "\n",
    "#print the amount of each tag found in the text, print that in a table using pandas\n",
    "\n",
    "print(\"\\n\\n\\n\\nCantidad de etiquetas encontradas en el texto:\")\n",
    "for tag in tag_to_ix:\n",
    "    count = 0\n",
    "    for sent, tags in training_data + test_data:\n",
    "        for word in tags:\n",
    "            if word == tag:\n",
    "                count += 1\n",
    "    print(tag,\" : \", count)\n",
    "\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    #put the rest of i in other list\n",
    "    if len(training_data[i][1]) == 6:\n",
    "        print(training_data[i])\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    #put the rest of i in other list\n",
    "    if len(test_data[i][1]) == 6:\n",
    "        print(test_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardÃ\\xadn']\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de procesamiento de una oración\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(training_data[0][0])                          \n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLmaT08_DfVm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'negro', 'corre', 'en', 'el', 'jardÃ\\xadn']\n",
      "tensor([[-2.0294, -1.4995, -1.4343, -1.7211, -1.4776],\n",
      "        [-2.1721, -1.5072, -1.4454, -1.7485, -1.3671],\n",
      "        [-2.1510, -1.3367, -1.5995, -1.6185, -1.5107],\n",
      "        [-2.1193, -1.3589, -1.5976, -1.6407, -1.4842],\n",
      "        [-2.0679, -1.4440, -1.6056, -1.6738, -1.3892],\n",
      "        [-2.1571, -1.3371, -1.6179, -1.6046, -1.5030],\n",
      "        [-2.0911, -1.4176, -1.5764, -1.6657, -1.4338]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (7).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 112\u001b[0m\n\u001b[0;32m    108\u001b[0m tag_scores \u001b[39m=\u001b[39m model(sentence_in)\n\u001b[0;32m    110\u001b[0m \u001b[39m# Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m# parámetros por medio del optimizador.\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m loss \u001b[39m=\u001b[39m loss_function(tag_scores, targets)\n\u001b[0;32m    113\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m    114\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2704\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (7)."
     ]
    }
   ],
   "source": [
    "# Definición del modelo\n",
    "\n",
    "# El modelo es una clase que debe heredar de nn.Module\n",
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    Clase para aplicar POST a oraciones en español. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Incialización del modelo\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        \"\"\"\n",
    "        Inicialización de la clase.\n",
    "        Parámetros:\n",
    "           embedding_dim: dimesionalidad del vector de palabras. \n",
    "           hidden_dim: dimensión de la capa oculta de la red. \n",
    "           vocab_size: tamaño del vocabulario.  \n",
    "           tagset_size: número de clases.\n",
    "        \"\"\"\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Primero se pasa la entrada a través de una capa Embedding. \n",
    "        # Esta capa construye una representación de los tokens de \n",
    "        # un texto donde las palabras que tienen el mismo significado \n",
    "        # tienen una representación similar.\n",
    "        \n",
    "        # Esta capa captura mejor el contexto y son espacialmente \n",
    "        # más eficientes que las representaciones vectoriales (one-hot vector).\n",
    "        # En Pytorch, se usa el módulo nn.Embedding para crear esta capa, \n",
    "        # que toma el tamaño del vocabulario y la longitud deseada del vector \n",
    "        # de palabras como entrada. Ejemplos en [3] y [4]\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # El LSTM toma word_embeddings como entrada y genera los estados ocultos\n",
    "        # con dimensionalidad hidden_dim.  \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # La capa lineal mapea el espacio de estado oculto \n",
    "        # al espacio de clases\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Pasada hacia adelante de la red. \n",
    "        # Parámetros:\n",
    "        #    sentence: la oración a procesar\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "\n",
    "        # Se utiliza softmax para devolver la probabilidad de cada etiqueta\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "\n",
    "#============================================================\n",
    "#BORRAR AL FINAL DESDE AQUI O MOVERLAS A CELDAS POR SEPARADO  \n",
    "#============================================================\n",
    "\n",
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "EMBEDDING_DIM = 6\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Generalmente utilizada en problemas de clasificacion con múltiples clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#============================================================\n",
    "#BORRAR AL FINAL DESDE AQUI O MOVERLAS A CELDAS POR SEPARADO\n",
    "#============================================================\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    print(training_data[0][0])\n",
    "    \n",
    "    # Clasificación    \n",
    "    print(tag_scores)\n",
    "\n",
    "# Épocas de entrenamiento\n",
    "for epoch in range(200):  \n",
    "    for sentence, tags in training_data:\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "    # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "    # por eso el tensor de salida tiene 4 elementos. \n",
    "    # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "    # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "    # la posición del valor máximo\n",
    "    print(tag_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHhOk08h1qY2"
   },
   "outputs": [],
   "source": [
    "# Instanciación del modelo, definición de la función de pérdida y del optimizador   \n",
    "\n",
    "# Hiperparámetros de la red\n",
    "# Valores generalmente altos (32 o 64 dimensiones).\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Instancia del modelo\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "\n",
    "# Función de pérdida: Negative Log Likelihood Loss (NLLL). \n",
    "# Generalmente utilizada en problemas de clasificacion con múltiples clases.\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Optimizador Stochastic Gradient Descent  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "bRYCyKy-DfVu",
    "outputId": "5ccdff4d-dcca-4545-81bf-69a396ec4fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['el', 'coche', 'rojo', 'esta', 'en', 'la', 'calle']\n",
      "tensor([[-1.7479, -1.5793, -1.5727, -1.4541, -1.7222],\n",
      "        [-1.7488, -1.5726, -1.5877, -1.4649, -1.6982],\n",
      "        [-1.7441, -1.4946, -1.5705, -1.4701, -1.8140],\n",
      "        [-1.7620, -1.5551, -1.5399, -1.4576, -1.7724],\n",
      "        [-1.7356, -1.5184, -1.5611, -1.5095, -1.7503],\n",
      "        [-1.7388, -1.6100, -1.5663, -1.5102, -1.6363],\n",
      "        [-1.7565, -1.5474, -1.5551, -1.4710, -1.7504]])\n",
      "Resultados luego del entrenamiento para la primera frase\n",
      "tensor([[-3.6377e-03, -6.2140e+00, -9.4082e+00, -1.1041e+01, -6.4813e+00],\n",
      "        [-7.1440e+00, -4.1712e-03, -9.8455e+00, -5.7280e+00, -9.6226e+00],\n",
      "        [-1.1161e+01, -5.9205e+00, -4.2425e+00, -1.7274e-02, -9.7956e+00],\n",
      "        [-8.8833e+00, -9.1328e+00, -1.1745e-02, -5.0667e+00, -5.2734e+00],\n",
      "        [-5.9632e+00, -6.8017e+00, -5.1643e+00, -9.7382e+00, -9.5044e-03],\n",
      "        [-5.7834e-03, -6.0287e+00, -1.1228e+01, -1.3142e+01, -5.7010e+00],\n",
      "        [-8.2321e+00, -5.3538e-03, -8.8830e+00, -5.3395e+00, -8.8985e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo \n",
    "\n",
    "# Valores antes de entrenar\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    \n",
    "    print(training_data[0][0])\n",
    "    \n",
    "    # Clasificación    \n",
    "    print(tag_scores)\n",
    "\n",
    "# Épocas de entrenamiento\n",
    "for epoch in range(200):  \n",
    "    for sentence, tags in training_data:\n",
    "        ## Paso 1. Pytorch acumula los gradientes.\n",
    "        # Es necesario limpiarlos\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Paso 2. Se preparan las entradas, es decir, se convierten a\n",
    "        # tensores de índices de palabras.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Paso 3. Se genera la predicción (forward pass).\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Paso 4. se calcula la pérdida, los gradientes, y se actualizan los \n",
    "        # parámetros por medio del optimizador.\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Despligue de la puntuación luego del entrenamiento\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "   \n",
    "    print(\"Resultados luego del entrenamiento para la primera frase\")\n",
    "    # Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "    # La primera oración tiene 4 palabras \"El perro come manzana\"\n",
    "    # por eso el tensor de salida tiene 4 elementos. \n",
    "    # Cada elemento es un vector de pesos que indica cuál etiqueta tiene más\n",
    "    # posibilidad de estar asociada a la palabra. Es decir hay que calcular \n",
    "    # la posición del valor máximo\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "sPj6v4tsDfV2",
    "outputId": "6e5d5197-c82f-4d22-c74c-cb22c5b20af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases\n",
      "{'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n",
      "FRASE\n",
      "La frase original ['el', 'viento', 'fresco', 'sopla', 'entre', 'los', 'arboles']\n",
      "La frase original preprocesada tensor([ 0, 61, 62, 63, 64, 59, 65])\n",
      "Salida del modelo tensor([[-3.6377e-03, -6.2140e+00, -9.4082e+00, -1.1041e+01, -6.4813e+00],\n",
      "        [-7.0259e+00, -7.0815e-03, -9.1450e+00, -5.1159e+00, -9.7112e+00],\n",
      "        [-4.2740e+00, -2.2147e+00, -2.7350e+00, -2.1087e-01, -6.1577e+00],\n",
      "        [-7.2489e+00, -7.7990e+00, -2.2352e-02, -4.3518e+00, -4.8159e+00],\n",
      "        [-2.3588e+00, -4.6996e+00, -2.0706e+00, -5.2541e+00, -2.6784e-01],\n",
      "        [-2.8370e-02, -4.0488e+00, -9.3408e+00, -1.0269e+01, -4.5654e+00],\n",
      "        [-5.3606e+00, -7.3568e-03, -9.9458e+00, -6.0183e+00, -8.8042e+00]])\n",
      "Valores máximos e índices (tensor([-0.0036, -0.0071, -0.2109, -0.0224, -0.2678, -0.0284, -0.0074]), tensor([0, 1, 3, 2, 4, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Uso del modelo generado\n",
    "\n",
    "def test_examples(test_data):\n",
    "\n",
    "   with torch.no_grad():\n",
    "      inputs = prepare_sequence(test_data, word_to_ix)\n",
    "      tag_scores = model(inputs)\n",
    "    \n",
    " \n",
    "   print(\"FRASE\") \n",
    "   print(\"La frase original\", test_data)    \n",
    "   print(\"La frase original preprocesada\", inputs)\n",
    "   print(\"Salida del modelo\", tag_scores)\n",
    "   print(\"Valores máximos e índices\", max_values(tag_scores))    \n",
    "    \n",
    "print(\"Clases\")\n",
    "print(tag_to_ix)\n",
    "\n",
    "#Frase 1\n",
    "# Las palabras en una oración se pueden etiquetar de tres formas.\n",
    "# La primera oración tiene 3 palabras \"El perro juega\"\n",
    "# por eso el tensor de salida tiene 3 elementos. \n",
    "# Cada elemento es un vector de probabilidad de estar asociada a una clase. \n",
    "# Es decir hay que calcular la posición del valor máximo. \n",
    "#   Ejemplo 1: \"El perro juega\" [\"DET\", \"NN\", \"V\"]\n",
    "# Ejemplo: salida 0, 1, 2 con {\"DET\": 0, \"NN\": 1, \"V\": 2} => DET, NN, V \n",
    "test_examples(test_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASE\n",
      "La frase original ['el', 'sol', 'radiante', 'brilla', 'en', 'el', 'horizonte']\n",
      "La frase original preprocesada tensor([ 0, 69, 70, 71,  4,  0, 72])\n",
      "Salida del modelo tensor([[-3.6377e-03, -6.2140e+00, -9.4082e+00, -1.1041e+01, -6.4813e+00],\n",
      "        [-4.6229e+00, -1.1412e-02, -1.0982e+01, -6.5131e+00, -1.0709e+01],\n",
      "        [-5.1358e+00, -1.4691e+00, -4.6898e+00, -2.8190e-01, -7.7391e+00],\n",
      "        [-2.6726e+00, -5.5529e+00, -1.2200e-01, -3.4113e+00, -4.7213e+00],\n",
      "        [-3.5128e+00, -3.9415e+00, -4.4128e+00, -7.6311e+00, -6.3831e-02],\n",
      "        [-3.0990e-03, -6.7913e+00, -1.0732e+01, -1.2591e+01, -6.2423e+00],\n",
      "        [-8.7508e+00, -7.9279e-03, -8.5537e+00, -4.8941e+00, -9.8154e+00]])\n",
      "Valores máximos e índices (tensor([-0.0036, -0.0114, -0.2819, -0.1220, -0.0638, -0.0031, -0.0079]), tensor([0, 1, 3, 2, 4, 0, 1]))\n",
      "valor de las etiquetas {'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n"
     ]
    }
   ],
   "source": [
    "#Frase 2\n",
    "test_examples(test_data[3][0])\n",
    "\n",
    "print(\"valor de las etiquetas\", tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRASE\n",
      "La frase original ['el', 'agua', 'clara', 'fluye', 'por', 'el', 'rio']\n",
      "La frase original preprocesada tensor([ 0, 67, 68, 46, 32,  0, 44])\n",
      "Salida del modelo tensor([[-3.6377e-03, -6.2140e+00, -9.4082e+00, -1.1041e+01, -6.4813e+00],\n",
      "        [-6.7706e+00, -1.2532e-02, -9.1952e+00, -4.5002e+00, -9.2278e+00],\n",
      "        [-7.7423e+00, -3.7200e+00, -5.3346e+00, -3.0255e-02, -8.0744e+00],\n",
      "        [-9.4140e+00, -8.6919e+00, -1.9223e-02, -4.2027e+00, -5.5637e+00],\n",
      "        [-5.6938e+00, -7.2577e+00, -4.9830e+00, -9.4669e+00, -1.1064e-02],\n",
      "        [-3.2324e-03, -7.3515e+00, -1.0614e+01, -1.2878e+01, -5.9684e+00],\n",
      "        [-7.8867e+00, -3.5831e-03, -9.3864e+00, -5.7904e+00, -9.7164e+00]])\n",
      "Valores máximos e índices (tensor([-0.0036, -0.0125, -0.0303, -0.0192, -0.0111, -0.0032, -0.0036]), tensor([0, 1, 3, 2, 4, 0, 1]))\n",
      "valor de las etiquetas {'DET': 0, 'NN': 1, 'V': 2, 'ADJ': 3, 'PREP': 4}\n"
     ]
    }
   ],
   "source": [
    "# Otra prueba\n",
    "test_examples(test_data[2][0])\n",
    "print(\"valor de las etiquetas\", tag_to_ix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZjYZ9vfDfV9"
   },
   "source": [
    "# Referencias \n",
    "\n",
    "[1] Guthrie, R. (2017). Tutorial. Sequence Models and Long-Short Term Memory Networks. Recuperado de https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "[2] LeCun,Y., Bengio, Y.,  & Hinton, G. (2015). Deep learning. Nature, 521(7553):436.\n",
    "\n",
    "[3] Brownlee, J. (2017). What Are Word Embeddings for Text?. Recuperado de https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "[4] Bishop, C (2006). Pattern Recognition and Machine Learning. Springer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "16_POST_with_LSTM_(Esp).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
